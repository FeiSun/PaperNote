#### Improving the Transformer Translation Model with Document-Level Context

* Using multi-head to encode document context for improving Transformer
* But with only preceding two sentences!



#### Context-Aware Self-Attention Networks

* enhance selt attention using context information



#### Self-Attention with Relative Position Representations

* enhance self attention using relative position vector



#### Distance-based Self-Attention Network for Natural Language Inference

* add distance mask into the softmax in self-attention
* Such simple trick works



#### Self-Attention: A Better Building Block for Sentiment Analysis Neural Network Classifiers

* relative position self-attention is better







